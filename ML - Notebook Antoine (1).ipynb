{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Machine Learning Project: Electricity Price Explanation ‚öôÔ∏èüìâ\n",
    "\n",
    "**LANOTTE-MORO Alix, CHANDECLERC Antoine, GILLES Julien, FLEURY Nathan**  \n",
    "**November 14, 2024**\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project focuses on analyzing daily variations in electricity futures prices within the European market, specifically for France and Germany. Given the complex interplay of meteorological, energy production, and geopolitical factors impacting price fluctuations, the objective is to build a model that explains these movements rather than predicting exact prices. Leveraging Spearman correlation as the primary evaluation metric, we will employ a range of models‚Äîfrom linear regression benchmarks to advanced machine learning approaches (e.g., Random Forests, XGBoost, and RNNs)‚Äîto capture both linear and non-linear influences on price variations. This model aims to support energy producers, traders, and policymakers by providing insights to optimize trading strategies, manage risks, and strengthen the resilience of the European energy grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importation and first preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df =pd.read_csv('X_train_NHkHMNU.csv', delimiter= ',')\n",
    "y_df =pd.read_csv('y_train_ZAN5mwg.csv', delimiter= ',')\n",
    "X_test_df =pd.read_csv('X_test_final.csv', delimiter= ',')\n",
    "\n",
    "df = pd.merge(X_df,y_df,on='ID')\n",
    "\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourcentage of missing values by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_missing = df.isna().sum()\n",
    "rate_missing = nb_missing / df.ID.nunique()\n",
    "fig, ax = plt.subplots(figsize=(4,6))\n",
    "ax1 = ax\n",
    "rate_missing.plot(kind=\"barh\", ax=ax1)\n",
    "ax1.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there are a certain number of missing values, particularly for DE_NET_IMPORT and DE_NET_EXPORT, where the rate exceeds 8%. These data will need to be handled. Several options are available to us: filling them with the mean, replacing them with zeros, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to fill the missing values in each column with the respective column mean. Since the dataset is relatively small (around 1500 rows), dropping rows with missing values would significantly reduce the available data. By imputing missing values with the mean, we retain as many rows as possible, ensuring that the dataset remains comprehensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of Columns with missing values\n",
    "columns_to_fill = [\n",
    "    \"DE_FR_EXCHANGE\", \"FR_DE_EXCHANGE\", \"DE_NET_EXPORT\", \"FR_NET_EXPORT\", \n",
    "    \"DE_NET_IMPORT\", \"FR_NET_IMPORT\", \"DE_RAIN\", \"FR_RAIN\", \n",
    "    \"DE_WIND\", \"FR_WIND\", \"DE_TEMP\", \"FR_TEMP\"\n",
    "]\n",
    "df_general = df\n",
    "\n",
    "#Fill missing values with the mean\n",
    "for column in columns_to_fill:\n",
    "    if column in df_general.columns:\n",
    "        df_general[column].fillna(df_general[column].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the distribution of each columns of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature for feature in df.columns if feature != \"COUNTRY\"]\n",
    "\n",
    "nb_col = 6\n",
    "nb_row = - (-len(features)//6)\n",
    "fig, ax = plt.subplots(nb_row, nb_col, figsize=(14,14))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    i_col = i % nb_col\n",
    "    i_row = i // nb_col\n",
    "    ax1 = ax[i_row, i_col]\n",
    "    \n",
    "    ax1.set_title(feature)\n",
    "    ax1.grid()\n",
    "    df[feature].hist(bins= 30, ax=ax1, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax1 = ax\n",
    "y_df[\"TARGET\"].hist(bins= 30, ax=ax1, alpha=0.7)\n",
    "ax1.set_title(\"TARGET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution data points by country\n",
    "print(\"Nb of data points by country:\")\n",
    "print(df.COUNTRY.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to predict prices for France and Germany. To better understand the factors influencing price in each country, we will plot the data separately. This allows us to evaluate whether the factors driving price variations are consistent or differ between the two countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the data by country, drop the country column and fill missing values with the mean\n",
    "df_de = df[df['COUNTRY'] == 'DE']\n",
    "df_de = df_de.drop(columns=['COUNTRY'])\n",
    "df_de.fillna(df_de.mean(), inplace=True)\n",
    "\n",
    "df_fr = df[df.COUNTRY == \"FR\"]\n",
    "df_fr = df_fr.drop(columns=['COUNTRY'])\n",
    "df_fr.fillna(df_fr.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the distribution of each column by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_col = 6\n",
    "nb_row = - (-len(features)//6)\n",
    "fig, ax = plt.subplots(nb_row, nb_col, figsize=(14,14))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    i_col = i % nb_col\n",
    "    i_row = i // nb_col\n",
    "    ax1 = ax[i_row, i_col]\n",
    "    \n",
    "    ax1.set_title(feature)\n",
    "    ax1.grid()\n",
    "    df_fr[feature].hist(bins= 30, ax=ax1, alpha=0.7, label= \"FR\")\n",
    "    df_de[feature].hist(bins= 30, ax=ax1, alpha=0.6, label= \"DE\")\n",
    "    ax1.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the **distributions of various features** are generally similar between **France** and **Germany**. However, it is notable that the data associated with **Germany** exhibit greater **variability** across several variables. This can be attributed to a more **comprehensive** and **dense database** on the French side.  \n",
    "\n",
    "It could be relevant to separate the data into **two distinct sets** to develop **specific models** for each country. Indeed, the **economic differences** between France and Germany are significant and can influence their respective behaviors, particularly regarding **electricity prices**. These differences are attributable to several factors, including:  \n",
    "\n",
    "- **The structure of the energy mix**: France relies heavily on **nuclear energy**, whereas Germany has gradually reduced its use of nuclear energy in favor of **renewable sources** such as wind and solar. This creates differing dynamics in electricity supply and **sensitivity to weather conditions**.  \n",
    "\n",
    "- **Energy policies**: The energy strategies differ between the two countries, with a faster energy transition in **Germany**, accompanied by **subsidy and regulatory policies** to promote renewables. In France, dependence on nuclear energy leads to a policy focus on **supply stability**.  \n",
    "\n",
    "- **Climatic and geographical conditions**: Climatic variations, such as differences in **temperature** and **precipitation**, differently affect energy consumption in each country. For instance, France may experience higher heating demand in winter, while Germany, with its significant **wind power infrastructure**, is more impacted by wind variations.  \n",
    "\n",
    "These differences suggest that a single model might not adequately capture the unique dynamics of each country. Therefore, an approach using **separate models** for **France** and **Germany** could better reflect their economic and energy-specific characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Data Availability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = df.DAY_ID.value_counts().sort_index()\n",
    "F = F.reindex(range(F.index.max()))\n",
    "F = F.fillna(0)\n",
    "F = F.value_counts()\n",
    "F /= F.sum()\n",
    "fig, ax = plt.subplots(figsize=(4,1))\n",
    "ax1 = ax\n",
    "ax1.set_title(\"Number of points per day in time period\")\n",
    "F.plot(kind=\"barh\", ax=ax1)\n",
    "ax1.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of days based on data availability (53% of days with data for both countries, 30% without data, and the remainder with partial data available for only one of the two countries) provides important insights into the quality and representativeness of the sample used in our analysis.  \n",
    "\n",
    "This distribution highlights the need to adopt strategies for handling missing data. Two approaches can be considered:  \n",
    "\n",
    "- **Separate models by country** for days where data is available for only one country.  \n",
    "- **Imputation or exclusion of days without data**, depending on their proportion and the potential impact on the analysis.  \n",
    "\n",
    "These observations call for caution when interpreting the results, as irregular data availability may affect conclusions regarding temporal phenomena or direct comparisons between France and Germany.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=\"COUNTRY\", y=\"TARGET\", data=df)\n",
    "plt.title(\"Distribution of the target variable 'TARGET' by country\")\n",
    "plt.xlabel(\"COUNTRY\")\n",
    "plt.ylabel(\"TARGET\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the distribution of the target variable for both countries reveals that the median is similar and close to zero for both France and Germany. This suggests that the target values are well **centered around zero** in both cases, indicating a generally balanced distribution of positive and negative values. This centrality can be interpreted as follows:  \n",
    "\n",
    "- **Absence of trend bias**: The proximity of the median to zero for both countries suggests there is no systematic bias in the target values. In other words, the values do not skew significantly toward positive or negative levels, which is favorable for comparative analyses or symmetric predictive models.  \n",
    "- **Quantile**: The interquartile range is slightly narrower for France, meaning that most TARGET values are more concentrated around the median compared to Germany. However, the presence of numerous outliers in Francesignificantly extends the range of extreme values, potentially requiring special handling to prevent undue influence on the models.  \n",
    "\n",
    "#### Presence of Outliers  \n",
    "\n",
    "Another striking aspect is the presence of **numerous outliers in the French data**, whereas extreme values are less frequent in Germany. This could reflect several dynamics:  \n",
    "\n",
    "- **Increased variability in French data**: The higher frequency of extreme values in France might indicate greater volatility in the market or factors influencing the target variable.  \n",
    "- **Influence of country-specific conditions**: France and Germany differ in infrastructure and energy mix (e.g., France's heavier reliance on nuclear energy compared to Germany). These structural differences could lead to extreme variations in certain energy indicators, especially during crises or periods of exceptional demand.  \n",
    "- **Potential impact on predictive models**: These outliers could complicate modeling for the French data. Extreme values might influence the mean and variance, which could affect models sensitive to significant deviations. Techniques for managing outliers (e.g., data transformation or smoothing extreme values) may be necessary to enhance model robustness.  \n",
    "\n",
    "#### Implications for Analysis  \n",
    "\n",
    "The observations regarding centrality and outliers have several implications for analysis and modeling:  \n",
    "\n",
    "1. **Strategies for managing extreme values**: The outliers in the French data may require special handling, such as outlier removal or the application of normalization or logarithmic transformation techniques to reduce their impact.  \n",
    "2. **Country-specific approaches**: Since the target variable's distribution differs between France and Germany (particularly regarding outliers), it may be advisable to build separate models or apply differentiated preprocessing for each country.  \n",
    "3. **Use of robust models**: In the presence of numerous outliers, more robust models, such as those based on the median (quantile regression) or ensemble algorithms (e.g., random forest or gradient boosting), might be better suited than standard linear models.  \n",
    "\n",
    "#### Summary  \n",
    "\n",
    "Although the French and German distributions of the target variable exhibit similar centrality, the higher frequency of outliers in France calls for a cautious approach. Proper handling of these outliers is essential to ensure the quality of results and the robustness of predictive models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson's Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "df_cleaned = df_general.drop(columns=['ID', 'DAY_ID','COUNTRY'])\n",
    "pearson_corr = df_cleaned.corr(method='pearson')\n",
    "sns.heatmap(pearson_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, annot_kws={\"size\": 8})\n",
    "plt.title(\"Pearson's Correlation Matrix of the initial dataset\")\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman's Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "df_cleaned = df.drop(columns=['ID', 'DAY_ID','COUNTRY'])\n",
    "spearman_corr = df_cleaned.corr(method='spearman')\n",
    "sns.heatmap(spearman_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, annot_kws={\"size\": 8})\n",
    "plt.title(\"Spearman's Correlation Matrix of the initial dataset\")   \n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the **correlation matrices**, it is noticeable that the **Spearman** matrix shows stronger correlations with the '**TARGET**' row compared to the **Pearson** matrix. This difference arises because **Spearman** evaluates the **monotonic relationship** between variables, whether linear or not, based on their **ranks** rather than their absolute values. This method better captures relationships when the data is **nonlinear** or contains **outliers**, which might distort the **Pearson** correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Production Distribution by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_sources = ['DE_COAL', 'FR_COAL', 'DE_GAS', 'FR_GAS', 'DE_NUCLEAR', 'FR_NUCLEAR', 'DE_WINDPOW', 'FR_WINDPOW']\n",
    "\n",
    "n_rows = len(energy_sources) // 2\n",
    "\n",
    "plt.figure(figsize=(14, n_rows * 5))\n",
    "\n",
    "for i, source in enumerate(energy_sources, 1):\n",
    "    plt.subplot(n_rows, 2, i)\n",
    "    sns.histplot(df_general[source], bins=30, kde=True)\n",
    "    plt.title(f'Histogram of {source}')\n",
    "    plt.xlabel(source)\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These charts demonstrate that while certain energy sources, such as **wind power**, appear to be similarly distributed between **France** and **Germany**, the two countries clearly adopt **distinct energy strategies**. Notably, **France** stands out for its greater **stability** in **nuclear energy** production, while it remains relatively **weak** in terms of **coal** production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman's Correlation Matrix for Data Related to France  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr_cleaned=df_fr.drop(['ID','DAY_ID'],axis=1)\n",
    "corr_matrix = df_fr_cleaned.corr()\n",
    "plt.figure(figsize=(16, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "spearman_corr = df_fr_cleaned.corr(method='spearman')\n",
    "sns.heatmap(spearman_corr, annot=True,mask=mask, fmt=\".2f\", cmap=\"magma\", cbar=True, annot_kws={\"size\": 8})\n",
    "plt.title(\"Matrice triangulaire de corr√©lation de Spearman\")\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman Correlation Matrix for Data Related to Germany "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_de_cleaned=df_de.drop(['ID','DAY_ID'],axis=1)\n",
    "corr_matrix = df_de_cleaned.corr()\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "spearman_corr = df_de_cleaned.corr(method='spearman')\n",
    "sns.heatmap(spearman_corr,mask=mask, annot=True, fmt=\".3f\", cmap=\"magma\", cbar=True, annot_kws={\"size\": 8})\n",
    "plt.title(\"Matrice triangulaire de corr√©lation de Spearman\")\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman's Correlation in Our Project\n",
    "\n",
    "In this project, we aim to explain daily variations in electricity futures prices in Europe. Given the complex and non-linear relationships between factors like energy production, weather conditions, and market prices, **Spearman's correlation** is a suitable tool to evaluate our model's performance, alongside other metrics like R¬≤, MAE, and MSE.\n",
    "\n",
    "### Why Use Spearman?\n",
    "\n",
    "1. **Monotonicity**: The relationships between variables, such as the impact of gas prices on electricity prices, may be monotonic but non-linear. Spearman captures these monotonic trends where Pearson would fail.\n",
    "\n",
    "2. **Robustness to Outliers**: Since the data can contain extreme values (e.g., sudden price spikes), Spearman is less sensitive to outliers, providing a more stable evaluation.\n",
    "\n",
    "3. **Capturing Non-linear Relationships**: Variables like renewable energy production or electricity exchanges may have a non-linear effect. Spearman helps measure the monotonic relationship, even if it's not strictly linear.\n",
    "\n",
    "### Spearman's Rank Correlation Formula\n",
    "\n",
    "The Spearman rank correlation coefficient ($\\rho$) is calculated as:\n",
    "\n",
    "$$\n",
    "\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $d_i$ is the difference between the ranks of corresponding values in the two datasets.\n",
    "- $n$ is the total number of data pairs.\n",
    "\n",
    "### Additional Evaluation Metrics\n",
    "\n",
    "- **R¬≤**: This metric tells us the proportion of the variance in the target variable (electricity prices) explained by the model. A higher R¬≤ indicates a better fit.\n",
    "  \n",
    "- **MAE (Mean Absolute Error)**: This measures the average magnitude of errors in predictions, providing insight into how close the model's predictions are to the actual values, with smaller values being preferable.\n",
    "  \n",
    "- **MSE (Mean Squared Error)**: This gives us the squared average of the model's prediction errors, emphasizing larger errors more heavily. Like MAE, smaller values indicate better model performance.\n",
    "\n",
    "Spearman's correlation is ideal for our project because it allows us to evaluate complex, non-linear trends in electricity prices while being robust to outliers. Alongside Spearman, we also use R¬≤, MAE, and MSE to provide a comprehensive view of our model's performance and ensure it captures the underlying dynamics of the electricity market effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation and Model Evaluation\n",
    "\n",
    "We use **k-fold cross-validation** to assess model performance on multiple data splits. The procedure involves:\n",
    "\n",
    "1. **Data Preparation**: Features (`X`) and target (`y`) are separated from the dataset.\n",
    "2. **K-Fold Cross-Validation**: Data is split into `k` folds, with each fold used for testing once.\n",
    "3. **Standardization**: Features are scaled to have zero mean and unit variance using **StandardScaler**.\n",
    "4. **Model Training**: The model is trained on the training data and predictions are made on the test set.\n",
    "5. **Evaluation Metrics**: We calculate **Spearman's correlation**, **R¬≤**, **MAE**, and **MSE** to assess model accuracy.\n",
    "6. **Result Aggregation**: Metrics from each fold are collected in a DataFrame and returned.\n",
    "\n",
    "This method provides a robust evaluation of the model‚Äôs performance across different subsets of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# Import timeseries split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def kfoldCrossValidation(df, M, k):\n",
    "    X = df.drop(columns=['ID', 'TARGET'], errors='ignore')\n",
    "    y = df['TARGET']\n",
    "\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    result = pd.DataFrame(columns=['Spearman', 'R2', 'MAE', 'MSE'])\n",
    "  \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "  \n",
    "    # Cross-validation loop\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]     \n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Data Standardization\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Training and prediction   \n",
    "        M.fit(X_train, y_train)\n",
    "        y_pred_test = M.predict(X_test)\n",
    "    \n",
    "        # Metrics calculation\n",
    "        result_temp = pd.DataFrame([{\n",
    "            'Spearman': spearmanr(y_test, y_pred_test).correlation, \n",
    "            'R2': M.score(X_test, y_test), \n",
    "            'MAE': np.mean(np.abs(y_test - y_pred_test)), \n",
    "            'MSE': np.mean((y_test - y_pred_test) ** 2)\n",
    "        }])\n",
    "    \n",
    "        if not result_temp.isna().all().all():  \n",
    "            result = pd.concat([result, result_temp], ignore_index=True)\n",
    "    \n",
    "    return result, y_test, y_pred_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Model Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have separate models for each country, a function is required to aggregate the predictions from these two distinct machine learning models, each designed specifically to predict electricity prices in France and Germany. Each model is developed independently to account for the unique market dynamics, energy sources, and economic factors in each country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Split the dataset by Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function split_dataset_by_country, which takes the original dataset as a parameter, enables splitting the dataset into two new datasets: df_fr (for the rows concerning France) and df_de (for the rows concerning Germany)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_by_country(df):\n",
    "    df_fr = df[df['COUNTRY'] == 'FR']\n",
    "    df_de = df[df['COUNTRY'] == 'DE']\n",
    "    return df_fr, df_de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Drop irrelevent Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided, for each country (and thus for each model), to focus on the columns specifically related to each country. The function drop_irrelevant_columns allows us to remove the columns related to Germany from the df_fr dataset and the columns related to France from the df_de dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_irrelevant_columns(df, country):\n",
    "    if country == 'FR':\n",
    "        columns_to_drop = ['COUNTRY', 'DE_CONSUMPTION', 'DE_FR_EXCHANGE', 'FR_NET_EXPORT',\n",
    "                           'DE_NET_EXPORT', 'DE_NET_IMPORT', 'DE_GAS', 'DE_COAL', 'DE_HYDRO',\n",
    "                           'DE_NUCLEAR', 'DE_SOLAR', 'DE_WINDPOW', 'DE_LIGNITE', 'DE_RESIDUAL_LOAD',\n",
    "                           'DE_RAIN', 'DE_WIND', 'DE_TEMP']\n",
    "    elif country == 'DE':\n",
    "        columns_to_drop = ['COUNTRY', 'FR_CONSUMPTION', 'FR_DE_EXCHANGE', 'DE_NET_EXPORT',\n",
    "                           'FR_NET_EXPORT', 'FR_NET_IMPORT', 'FR_GAS', 'FR_COAL', 'FR_HYDRO',\n",
    "                           'FR_NUCLEAR', 'FR_SOLAR', 'FR_WINDPOW', 'FR_RESIDUAL_LOAD',\n",
    "                           'FR_RAIN', 'FR_WIND', 'FR_TEMP']\n",
    "        \n",
    "    columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "    return df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fill Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset contains a certain number of missing values (as previously noted), and given the relatively limited amount of data, we couldn‚Äôt simply remove the rows with missing values. Therefore, we use the fill_missing_values function on df_fr and df_de individually to replace the missing values with the average of the column where the missing value is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    return df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merging the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we trained models and made predictions separately for France and Germany, the final model to submit at the end of the challenge is a single model. The function combine_results allows us to test the previous metrics, but this time on the entire set of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def combine_results(y_test_fr, y_pred_fr, y_test_de, y_pred_de):\n",
    "    y_pred_fr = pd.Series(y_pred_fr)\n",
    "    y_pred_de = pd.Series(y_pred_de)\n",
    "    y_test_de = pd.Series(y_test_de)\n",
    "    y_test_fr = pd.Series(y_test_fr)\n",
    "    \n",
    "    y_true_combined = pd.concat([y_test_fr, y_test_de], axis=0)\n",
    "    y_pred_combined = pd.concat([pd.Series(y_pred_fr), pd.Series(y_pred_de)], axis=0)\n",
    "    combined_r2 = r2_score(y_true_combined, y_pred_combined)\n",
    "    combined_mse = mean_squared_error(y_true_combined, y_pred_combined)\n",
    "    combined_spearman = spearmanr(y_true_combined, y_pred_combined).correlation\n",
    "    combined_mae = np.mean(np.abs(y_true_combined - y_pred_combined))   \n",
    "    return combined_r2, combined_mse, combined_mae, combined_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Final Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we implement our global function, which sequentially utilizes the previously defined functions (1 to 4). This function provides the model's performance metrics for France, Germany, and the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters \"FeatureEngineering\" and \"DropIrrelevantColumns\" will be applied later, and we will provide an explanation at that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_model(df, model_fr, model_de,FeatureEngineering=None, DropIrrelevantColumns=True):\n",
    "\n",
    "    # Step 1 : datasets splitting\n",
    "    df_fr, df_de = split_dataset_by_country(df)\n",
    "\n",
    "    if FeatureEngineering is None:\n",
    "        if DropIrrelevantColumns: \n",
    "            df_fr = drop_irrelevant_columns(df_fr, 'FR')\n",
    "            df_de = drop_irrelevant_columns(df_de, 'DE')\n",
    "        else:\n",
    "            df_fr = df_fr.drop(columns=['COUNTRY'])\n",
    "            df_de = df_de.drop(columns=['COUNTRY'])\n",
    "\n",
    "    else:\n",
    "        df_fr = df_fr.drop(columns=['COUNTRY'])\n",
    "        df_de = df_de.drop(columns=['COUNTRY'])\n",
    "        df_fr, df_de = FeatureEngineering(df_fr,df_de)\n",
    "\n",
    "    # Step 3 : Missing values replacement\n",
    "    df_fr = fill_missing_values(df_fr)\n",
    "    df_de = fill_missing_values(df_de)\n",
    "\n",
    "    result_fr, y_test_fr, y_pred_fr = kfoldCrossValidation(df_fr, model_fr, 6)\n",
    "    result_de, y_test_de, y_pred_de = kfoldCrossValidation(df_de, model_de, 6)\n",
    "\n",
    "    # Step 8 : Combining the results and evaluating\n",
    "    combined_r2, combined_mse, combined_mae, combined_spearman = combine_results(y_test_fr, y_pred_fr, y_test_de, y_pred_de)\n",
    "    result_global = pd.DataFrame([{\n",
    "        'R2': combined_r2,\n",
    "        'MSE': combined_mse,\n",
    "        'MAE': combined_mae,\n",
    "        'Spearman': combined_spearman\n",
    "    }])\n",
    "\n",
    "    return result_fr, result_de, result_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model Using Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model only with country separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on our prior efforts, it‚Äôs now straightforward to test and evaluate different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of a model using simple LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_basic = Final_model(df, LinearRegression(), LinearRegression(), DropIrrelevantColumns=False)\n",
    "result_basic[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our Spearman correlation, R¬≤, and MSE scores are relatively low. However, it's crucial to consider the context. In financial markets, price movements are influenced by thousands of factors with complex, non-linear relationships, making it nearly impossible to develop a model that fully explains electricity prices today. For reference, the best model from the ENS QRT challenge addressing this problem achieved a Spearman correlation of 0.6. Our objective is to enhance these evaluation metrics as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results by country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results for France:\")\n",
    "print(result_basic[0],\"\\n\")\n",
    "print(\"Mean results for France:\")\n",
    "result_basic[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results for Germany:\")\n",
    "print(result_basic[1],\"\\n\")\n",
    "print(\"Mean results for Germany:\")\n",
    "result_basic[1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed previously, it comes as no surprise that the German model outperforms the French one. This aligns with our earlier findings, where the features in the German dataset exhibited higher Spearman and Pearson correlation scores compared to those in the French dataset. Consequently, our model demonstrates better performance in Germany than in France."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with separation and deleting of useless column for each country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we explore whether the model's performance improves when features specific to France are removed for the German model, and vice versa. This is accomplished using the `DropIrrelevantColumns` function, which handles the removal of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Final_model(df,LinearRegression(), LinearRegression(), DropIrrelevantColumns=True)\n",
    "print(\"Results for the overall data:\")\n",
    "result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a slightly improved score after applying this approach, so we will retain it for the remainder of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for each Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results for France:\")\n",
    "print(result[0],\"\\n\")\n",
    "print(\"Mean results for France:\")\n",
    "result[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results for Germany:\")\n",
    "print(result[1],\"\\n\")\n",
    "print(\"Mean results for Germany:\")\n",
    "result[1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add more columns to create more precise models so we will do some feature engineering \n",
    "\n",
    "We can compute rolling statistics to capture trends and fluctuations over time. \n",
    "The rolling mean smooths the data to highlight long-term trends, while the rolling standard deviation measures volatility and variability within specific time windows (weekly and monthly). \n",
    "The slope, calculated using linear regression on rolling windows, captures the rate of change or trend direction, helping to identify whether values are generally increasing or decreasing over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate rolling statistics (mean, std, median, min, max, slope) for DE_ and FR_ columns\n",
    "def add_statistics(df, variables, windows):\n",
    "    def slope(y):\n",
    "        return np.polyfit(range(len(y)), y, 1)[0] if len(y) > 0 else np.nan\n",
    "    \n",
    "    df.order = df.sort_values(by='DAY_ID')\n",
    "    \n",
    "    for var in variables:\n",
    "        for window in windows:\n",
    "            for country in ['DE_', 'FR_']:\n",
    "                col = f'{country}{var}'\n",
    "                df[f'{col}_MEAN_{window}D'] = df[col].rolling(window=window).mean()\n",
    "                df[f'{col}_STD_{window}D'] = df[col].rolling(window=window).std()\n",
    "                df[f'{col}_MEDIAN_{window}D'] = df[col].rolling(window=window).median()\n",
    "                df[f'{col}_MIN_{window}D'] = df[col].rolling(window=window).min()\n",
    "                df[f'{col}_MAX_{window}D'] = df[col].rolling(window=window).max()\n",
    "                df[f'{col}_SLOPE_{window}D'] = df[col].rolling(window=window).apply(slope, raw=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use technical indicators widely use in finance like volatility and moving averages that are used to analyze time-series data. Volatility, typically measured by rolling standard deviation, helps quantify the level of fluctuation or uncertainty in the data. Moving averages, especially exponential moving averages (EMA), are used to smooth short-term fluctuations and highlight long-term trends or cycles, making it easier to detect underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicators(df):\n",
    "    for commodity in ['GAS_RET', 'COAL_RET', 'CARBON_RET']:\n",
    "        df[f'{commodity}_VOLATILITY_WEEKLY'] = df[commodity].rolling(window=7).std()\n",
    "        df[f'{commodity}_VOLATILITY_MONTHLY'] = df[commodity].rolling(window=30).std()\n",
    "        df[f'{commodity}_EMA_MONTHLY'] = df[commodity].ewm(span=30, adjust=False).mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can add column specific with this domain area such as energy ratios, weather effects, and clustering, wich will provide further insights into underlying patterns. Weather effects, such as temperature or wind, are crucial in energy demand prediction, as they directly influence consumption and production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add energy source ratios and effects\n",
    "def add_energy(df):\n",
    "    energy_sources = ['GAS', 'COAL', 'HYDRO', 'NUCLEAR', 'SOLAR', 'WINDPOW']\n",
    "    \n",
    "    for country in ['DE_', 'FR_']:\n",
    "        total_energy = sum(df[f'{country}{source}'] for source in energy_sources)\n",
    "        \n",
    "        for source in energy_sources:\n",
    "            df[f'{country}{source}_RATIO'] = df[f'{country}{source}'] / total_energy\n",
    "\n",
    "        df[f'{country}WIND_SOLAR'] = df[f'{country}WINDPOW'] + df[f'{country}SOLAR']\n",
    "        df[f'{country}TEMP_EFFECT'] = df[f'{country}TEMP'] * df[f'{country}CONSUMPTION']\n",
    "        df[f'{country}WIND_EFFECT'] = df[f'{country}WIND'] * df[f'{country}WINDPOW']\n",
    "        df[f'{country}SOLAR_EFFECT'] = (df[f'{country}SOLAR'] / df[f'{country}TEMP']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have rearrange all our functions to add columns we can create a single function that call the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to add custom features by combining all the previous functions\n",
    "def add_columns(df):\n",
    "    periods=[7, 30]# Weekly and Monthly\n",
    "    variables = ['CONSUMPTION', 'GAS', 'COAL', 'HYDRO', 'NUCLEAR', 'SOLAR', 'WINDPOW', 'TEMP', 'RAIN', 'WIND']\n",
    "\n",
    "    df=add_statistics(df, variables, periods)\n",
    "    df=add_energy(df)\n",
    "    df=add_indicators(df)\n",
    "    \n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function calculates the correlation matrix and selects features that are either highly correlated with each other or have low correlation with the target variable. It combines both conditions to decide which features to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_based_on_correlation(df, target_column, multicollinear_threshold, correlation_threshold):\n",
    "    # Calculate the Spearman correlation matrix because it is the metric that we choose\n",
    "    corr_matrix=df.corr(method='spearman')\n",
    "    # Identify columns that are highly correlated with each other\n",
    "    # (excluding the target variable correlation)\n",
    "    high_corr_var=np.where(corr_matrix > multicollinear_threshold)\n",
    "    high_corr_var=[(corr_matrix.index[x], corr_matrix.columns[y]) \n",
    "                     for x, y in zip(*high_corr_var) \n",
    "                     if x!=y and x < y]\n",
    "    # Extract the names of columns to drop based on multicollinearity\n",
    "    multicollinear_features=set([item for sublist in high_corr_var for item in sublist])\n",
    "    # Identify features that have a low correlation with the target variable\n",
    "    low_corr_with_target=corr_matrix[target_column][abs(corr_matrix[target_column]) < correlation_threshold].index.tolist()\n",
    "    # Combine features to drop due to multicollinearity and low correlation with target\n",
    "    features_to_drop=multicollinear_features.union(low_corr_with_target)\n",
    "    # Determine the final list of features to keep\n",
    "    features_to_keep=[feature for feature in df.columns if feature not in features_to_drop and feature!=target_column]\n",
    "    \n",
    "    return features_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we filter the dataset for France (COUNTRY == 'FR'), remove certain columns, and keep some of the original columns, which can still have an impact on our prediction even if they are less correlated than the ‚Äòartificial‚Äô columns from feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final function that will be used for the feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureEngineering(df_fr, df_de, df_fr_test=None, df_de_test=None):\n",
    "\n",
    "    # We retain the columns in the main dataset, but not the columns with a high correlation with the others or a lower correlation with TARGET\n",
    "    col_fr_best = select_features_based_on_correlation(df_fr, 'TARGET', 0.93, 0.08)\n",
    "\n",
    "    # Select columns that do not start with 'DE' (to avoid mixing German data)\n",
    "    df_no_de = df_fr[df_fr.columns[~df_fr.columns.str.startswith('DE')]]\n",
    "    col_fr = [col for col in df_no_de.columns if col in col_fr_best]\n",
    "    df_fr.fillna(df_fr.mean(), inplace=True)\n",
    "    df_fr_test.fillna(df_fr_test.mean(), inplace=True) if df_fr_test is not None else None\n",
    "\n",
    "    # We retain the columns in the main dataset, but not the columns with a high correlation with the others or a lower correlation with TARGET\n",
    "    col_de_best = select_features_based_on_correlation(df_de, 'TARGET', 0.93, 0.08)\n",
    "\n",
    "    # Select columns that do not start with 'FR' (to avoid mixing French data)\n",
    "    df_no_fr = df_de[df_de.columns[~df_de.columns.str.startswith('FR')]]\n",
    "    col_de = [col for col in df_no_fr.columns if col in col_de_best]\n",
    "    df_de.fillna(df_de.mean(), inplace=True)\n",
    "    df_de_test.fillna(df_de_test.mean(), inplace=True) if df_de_test is not None else None\n",
    "\n",
    "    original_fr = df_fr[col_fr]\n",
    "    original_de = df_de[col_de]\n",
    "\n",
    "    # Apply feature engineering\n",
    "    df_fr = add_columns(df_fr)\n",
    "    df_de = add_columns(df_de)\n",
    "\n",
    "    if df_fr_test is not None:\n",
    "        original_fr_test = df_fr_test[col_fr]\n",
    "        df_fr_test = add_columns(df_fr_test)\n",
    "    \n",
    "    if df_de_test is not None:\n",
    "        original_de_test = df_de_test[col_de]\n",
    "        df_de_test = add_columns(df_de_test)\n",
    "\n",
    "    # Select the best columns based on correlation\n",
    "    best_features_fr = select_features_based_on_correlation(df_fr, 'TARGET', 0.8, 0.08)\n",
    "    best_features_de = select_features_based_on_correlation(df_de, 'TARGET', 0.8, 0.15)\n",
    "\n",
    "    # Keep the selected columns while adding the original columns\n",
    "    df_fr = pd.concat([original_fr, df_fr[best_features_fr+['TARGET']]], axis=1)\n",
    "    df_de = pd.concat([original_de, df_de[best_features_de+['TARGET']]], axis=1)\n",
    "\n",
    "    df_fr_test = pd.concat([original_fr_test, df_fr_test[best_features_fr]], axis=1) if df_fr_test is not None else None\n",
    "    df_de_test = pd.concat([original_de_test, df_de_test[best_features_de]], axis=1) if df_de_test is not None else None\n",
    "\n",
    "    if df_fr_test is not None and df_de_test is not None:\n",
    "        return df_fr, df_de, df_fr_test, df_de_test\n",
    "    else:\n",
    "        return df_fr, df_de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Using LinearRegression and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Final_model(df,LinearRegression(), LinearRegression(),FeatureEngineering, DropIrrelevantColumns=True)\n",
    "result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Speaman Correlation improved a lot from 0.22 to almost 0.3. The features engineering give a better performance to our Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Using RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment with the **RandomForestRegressor**, which is highly effective for capturing non-linear relationships. To maximize its potential, we use **GridSearchCV**, a technique that systematically tests multiple combinations of hyperparameters for the Random Forest and selects the best configuration for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search_1 = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search_2 = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "result = Final_model(df, grid_search_1, grid_search_2, FeatureEngineering, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GridSearchCV_1 coeffcients :\", grid_search_1.best_params_)\n",
    "print(\"GridSearchCV_2 coeffcients :\", grid_search_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RandomForestRegressor** demonstrates a markedly higher Spearman Correlation score compared to **Linear Regression**, highlighting its enhanced ability to capture relationships within the data. Additionally, it achieves a notable improvement in both the **R¬≤ score** (0.06 versus 0.035) and **MSE**, further underscoring its effectiveness. These metrics indicate that the RandomForestRegressor is better equipped to capture underlying patterns, particularly non-linear relationships. The model‚Äôs higher R¬≤ and lower MSE reflect its superior capacity to explain variance in the data and deliver more accurate predictions. This performance emphasizes the advantages of employing a non-linear model like Random Forest, showcasing its robustness in predictive power and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, we plan to improve the model‚Äôs performance by fine-tuning the **RandomForestRegressor** and optimizing feature engineering specifically tailored to this model. These enhancements are expected to further reinforce its advantage over Linear Regression, particularly in terms of Spearman Correlation, by fully leveraging its ability to capture non-linear relationships between variables and the target. Additionally, we intend to investigate more advanced and sophisticated models to drive further gains in predictive accuracy and overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output for Challenge submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our project is part of a challenge, we need to apply our model to a X_test dataset and submit the predictions we make. The function below handles this process, ensuring that the model's predictions are properly generated and ready for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_for_submission(df, X_test_df,model_fr,model_de, feature_engineering = False):\n",
    "    \n",
    "    # Step 1 : datasets splitting\n",
    "    df_fr, df_de = split_dataset_by_country(df)\n",
    "    df_fr_test, df_de_test = split_dataset_by_country(X_test_df)\n",
    "\n",
    "    start_df_fr_test = df_fr_test.copy()\n",
    "    start_df_de_test = df_de_test.copy()\n",
    "\n",
    "\n",
    "    if feature_engineering == False:\n",
    "        #Step 2: Dropping unnecessary columns   \n",
    "        df_fr = drop_irrelevant_columns(df_fr, 'FR')\n",
    "        df_de = drop_irrelevant_columns(df_de, 'DE')\n",
    "        df_fr_test = drop_irrelevant_columns(df_fr_test, 'FR')\n",
    "        df_de_test = drop_irrelevant_columns(df_de_test, 'DE')\n",
    "\n",
    "    else:\n",
    "        df_fr = df_fr.drop(columns=['COUNTRY'])\n",
    "        df_de = df_de.drop(columns=['COUNTRY'])\n",
    "\n",
    "        df_fr_test = df_fr_test.drop(columns=['COUNTRY'])\n",
    "        df_de_test = df_de_test.drop(columns=['COUNTRY'])\n",
    "\n",
    "    # Step 3 : Missing values replacement\n",
    "    df_fr = fill_missing_values(df_fr)\n",
    "    df_de = fill_missing_values(df_de)\n",
    "\n",
    "    df_fr_test = fill_missing_values(df_fr_test)\n",
    "    df_de_test = fill_missing_values(df_de_test)\n",
    "\n",
    "    if(feature_engineering):\n",
    "        df_fr, df_de, df_fr_test, df_de_test = FeatureEngineering(df_fr,df_de, df_fr_test, df_de_test) \n",
    "\n",
    "    df_fr_scaled = StandardScaler().fit_transform(df_fr)\n",
    "    df_de_scaled = StandardScaler().fit_transform(df_de)\n",
    "    df_fr_scaled = pd.DataFrame(df_fr_scaled, columns=df_fr.columns)\n",
    "    df_de_scaled = pd.DataFrame(df_de_scaled, columns=df_de.columns)\n",
    "    df_fr_scaled.drop(columns=['TARGET'], inplace=True)\n",
    "    df_de_scaled.drop(columns=['TARGET'], inplace=True)\n",
    "\n",
    "    model_fr.fit(df_fr_scaled, df_fr['TARGET'])\n",
    "    model_de.fit(df_de_scaled, df_de['TARGET'])\n",
    "\n",
    "    df_fr_test_scaled = StandardScaler().fit_transform(df_fr_test)\n",
    "    df_de_test_scaled = StandardScaler().fit_transform(df_de_test)\n",
    "    df_fr_test_scaled = pd.DataFrame(df_fr_test_scaled, columns=df_fr_test.columns)\n",
    "    df_de_test_scaled = pd.DataFrame(df_de_test_scaled, columns=df_de_test.columns)\n",
    "\n",
    "    y_pred_fr = pd.DataFrame(model_fr.predict(df_fr_test_scaled))\n",
    "    y_pred_de = pd.DataFrame(model_de.predict(df_de_test_scaled))\n",
    "\n",
    "    y_pred_fr['ID'] = start_df_fr_test['ID'].values\n",
    "    y_pred_de['ID'] = start_df_de_test['ID'].values\n",
    "\n",
    "    #Rename columns\n",
    "    y_pred_fr.columns = ['TARGET', 'ID']\n",
    "    y_pred_de.columns = ['TARGET', 'ID']\n",
    "\n",
    "    #Change the order of columns for the submition\n",
    "    y_pred_fr = y_pred_fr[['ID', 'TARGET']]\n",
    "    y_pred_de = y_pred_de[['ID', 'TARGET']]\n",
    "\n",
    "    y_pred = pd.concat([y_pred_fr, y_pred_de], axis=0)\n",
    "   \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output_for_submission(df, X_test_df,LinearRegression(), LinearRegression(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future of the Project üöÄ\n",
    "\n",
    "The future of the project focuses on improving our model's performance and prediction accuracy. We plan to **fine-tune the RandomForest** üå≤, as it has the potential to capture non-linear relationships between the features and the target variable, which should give it a bigger edge over linear regression üìâ. \n",
    "\n",
    "Additionally, we will explore more sophisticated models üß†, such as **ensemble methods**, **gradient boosting**, and **neural networks** ü§ñ, to push the performance even further. As we continue, we‚Äôll refine the **feature engineering** üîß process, optimize **hyperparameters** ‚öôÔ∏è, and incorporate **domain-specific insights** üìä to enhance the model‚Äôs robustness and reliability.\n",
    "\n",
    "Our ultimate goal üéØ is to create a predictive model that can effectively handle the complex dynamics of the problem and achieve higher accuracy in forecasting üåü.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_best_grid_parameters(data,model,parametre, FeatureEngineering=None, DropIrrelevantColumns=True):\n",
    "\n",
    "    # Step 1 : datasets splitting\n",
    "    df_fr, df_de = split_dataset_by_country(data)\n",
    "\n",
    "    if FeatureEngineering is None:\n",
    "        if DropIrrelevantColumns: \n",
    "            df_fr = drop_irrelevant_columns(df_fr, 'FR')\n",
    "            df_de = drop_irrelevant_columns(df_de, 'DE')\n",
    "        else:\n",
    "            df_fr = df_fr.drop(columns=['COUNTRY'])\n",
    "            df_de = df_de.drop(columns=['COUNTRY'])\n",
    "\n",
    "    else:\n",
    "        df_fr = df_fr.drop(columns=['COUNTRY'])\n",
    "        df_de = df_de.drop(columns=['COUNTRY'])\n",
    "        df_fr, df_de = FeatureEngineering(df_fr,df_de)\n",
    "\n",
    "    # Step 3 : Missing values replacement\n",
    "    df_fr = fill_missing_values(df_fr)\n",
    "    df_de = fill_missing_values(df_de)\n",
    "    \n",
    "    grid1= GridSearchCV(estimator=model, param_grid=parametre, cv=5, n_jobs=1)\n",
    "    grid2= GridSearchCV(estimator=model, param_grid=parametre, cv=5, n_jobs=1)\n",
    "    \n",
    "    X_fr=df_fr.drop('TARGET', axis=1)\n",
    "    X_de=df_de.drop('TARGET', axis=1)\n",
    "    y_fr=df_fr['TARGET']\n",
    "    y_de=df_de['TARGET']\n",
    "    \n",
    "    #X_train_fr, X_test_fr, y_train_fr, y_test_fr = train_test_split(X_fr, y_fr, test_size=0.2, random_state=42)\n",
    "    #X_train_de, X_test_de, y_train_de, y_test_de = train_test_split(X_de, y_de, test_size=0.2, random_state=42)\n",
    "    \n",
    "    grid1.fit(X_fr,y_fr)\n",
    "    grid2.fit(X_de,y_de)\n",
    "    \n",
    "    best_params_fr = grid1.best_params_\n",
    "    best_spearman_fr = grid1.best_score_\n",
    "    \n",
    "    best_params_de = grid2.best_params_\n",
    "    best_spearman_de = grid2.best_score_\n",
    "    \n",
    "    print(\"Meilleur parametre pour la France : \",best_params_fr)\n",
    "    print(\"Meilleur spearman pour la France : \",best_spearman_fr)\n",
    "    print(\"Meilleur parametre pour l'Allemange : \",best_params_de)\n",
    "    print(\"Meilleur spearman pour l'Allemagne : \",best_spearman_de)\n",
    "    \n",
    "    return best_spearman_fr, best_params_fr,best_spearman_de, best_params_de\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es\n",
    "X_df = pd.read_csv('X_train_NHkHMNU.csv', delimiter=',')\n",
    "y_df = pd.read_csv('y_train_ZAN5mwg.csv', delimiter=',')\n",
    "\n",
    "# Fusion des donn√©es d'entra√Ænement sur la colonne 'ID'\n",
    "df = pd.merge(X_df, y_df, on='ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "best_spearman_fr, best_params_fr,best_spearman_de, best_params_de = get_best_grid_parameters(df,rf,param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.stats import spearmanr  # Pour calculer la corr√©lation de Spearman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectivefr(trial):\n",
    "    # 1. Sugg√©rer des hyperparam√®tres\n",
    "    lr = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "\n",
    "    # 2. Cr√©er et entra√Æner le mod√®le\n",
    "    model = create_nn_model(input_dim=X_train_fr.shape[1], learning_rate=lr)\n",
    "    model.fit(X_train_fr, y_train_fr, batch_size=batch_size, epochs=10, verbose=0)\n",
    "\n",
    "    # 3. Faire des pr√©dictions sur les donn√©es de test\n",
    "    y_pred_fr = model.predict(X_test_fr).flatten()\n",
    "\n",
    "    # 4. Calculer la corr√©lation de Spearman\n",
    "    spearman_corr, _ = spearmanr(y_test_fr, y_pred_fr)\n",
    "\n",
    "    # Optuna maximise la valeur, donc on retourne -spearman_corr pour minimiser\n",
    "    return -spearman_corr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectivede(trial):\n",
    "    # 1. Sugg√©rer des hyperparam√®tres\n",
    "    lr = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "\n",
    "    # 2. Cr√©er et entra√Æner le mod√®le\n",
    "    model = create_nn_model(input_dim=X_train_de.shape[1], learning_rate=lr)\n",
    "    model.fit(X_train_de, y_train_de, batch_size=batch_size, epochs=10, verbose=0)\n",
    "\n",
    "    # 3. Faire des pr√©dictions sur les donn√©es de test\n",
    "    y_pred_de = model.predict(X_test_de).flatten()\n",
    "\n",
    "    # 4. Calculer la corr√©lation de Spearman\n",
    "    spearman_corr, _ = spearmanr(y_test_de, y_pred_de)\n",
    "\n",
    "    # Optuna maximise la valeur, donc on retourne -spearman_corr pour minimiser\n",
    "    return -spearman_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_dim, learning_rate):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le de r√©seau de neurones avec des param√®tres personnalisables.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Nombre de caract√©ristiques en entr√©e.\n",
    "        learning_rate (float): Taux d'apprentissage pour l'optimiseur.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Mod√®le de r√©seau de neurones compil√©.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))#on utilise relu pour expliquer des relations non-lin√©aires\n",
    "    model.add(Dense(32, activation='relu')) \n",
    "    model.add(Dense(1, activation='linear'))  # R√©gression (pr√©diction continue)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')# on optimise avec Adam\n",
    "    return model          #Adam est un algorithme d'optimisation utilis√© pour entra√Æner les r√©seaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es\n",
    "X_df = pd.read_csv('X_train_NHkHMNU.csv', delimiter=',')\n",
    "y_df = pd.read_csv('y_train_ZAN5mwg.csv', delimiter=',')\n",
    "\n",
    "# Fusion des donn√©es d'entra√Ænement sur la colonne 'ID'\n",
    "df = pd.merge(X_df, y_df, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour s√©parer les datasets par pays\n",
    "def split_dataset_by_country(dataframe):\n",
    "    df_fr = dataframe[dataframe['COUNTRY'] == 'FR']\n",
    "    df_de = dataframe[dataframe['COUNTRY'] == 'DE']\n",
    "    return df_fr, df_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©parer les datasets par pays\n",
    "df_fr, df_de = split_dataset_by_country(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr = fill_missing_values(df_fr)\n",
    "df_de = fill_missing_values(df_de)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©servation des jeux de test originaux pour v√©rification ult√©rieure\n",
    "#start_df_fr_test = df_fr_test.copy()\n",
    "#start_df_de_test = df_de_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr = drop_irrelevant_columns(df_fr, 'FR')\n",
    "df_de = drop_irrelevant_columns(df_de, 'DE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les cibles (TARGET) avant standardisation\n",
    "y_fr = df_fr.pop('TARGET')\n",
    "y_de = df_de.pop('TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des donn√©es\n",
    "scaler_fr = StandardScaler()\n",
    "scaler_de = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr_scaled = pd.DataFrame(scaler_fr.fit_transform(df_fr), columns=df_fr.columns)\n",
    "df_de_scaled = pd.DataFrame(scaler_de.fit_transform(df_de), columns=df_de.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration des caract√©ristiques et cibles pour les jeux d'entra√Ænement\n",
    "X_fr = df_fr_scaled\n",
    "X_de = df_de_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration des jeux d'entra√Ænement et de test (80% - 20%)\n",
    "X_train_fr, X_test_fr, y_train_fr, y_test_fr = train_test_split(X_fr, y_fr, test_size=0.2, random_state=42)\n",
    "X_train_de, X_test_de, y_train_de, y_test_de = train_test_split(X_de, y_de, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lancer Optuna pour l'optimisation\n",
    "study = optuna.create_study(direction='maximize')  # On minimise la MSE\n",
    "study.optimize(objectivefr, n_trials=100)  # 20 essais pour l'optimisation\n",
    "\n",
    "# 3. Afficher les meilleurs hyperparam√®tres et la meilleure corr√©lation de Spearman\n",
    "print(\"Meilleurs hyperparam√®tres pour la France :\", study.best_params)\n",
    "#print(\"Meilleure corr√©lation de Spearman :\", -study.best_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer Optuna pour l'optimisation\n",
    "study = optuna.create_study(direction='maximize')  # On minimise la MSE\n",
    "study.optimize(objectivede, n_trials=100)  # 20 essais pour l'optimisation\n",
    "\n",
    "# 3. Afficher les meilleurs hyperparam√®tres et la meilleure corr√©lation de Spearman\n",
    "print(\"Meilleurs hyperparam√®tres pour l'Allemgane :\", study.best_params)\n",
    "#print(\"Meilleure corr√©lation de Spearman :\", -study.best_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fr = create_nn_model(X_fr.shape[1],0.0001355209836354015)\n",
    "model_de = create_nn_model(X_de.shape[1],0.20411449016100183)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_fr.fit(X_train_fr, y_train_fr, epochs=20, batch_size=97, validation_data=(X_test_fr, y_test_fr), verbose=1)\n",
    "model_de.fit(X_train_de, y_train_de, epochs=20, batch_size=97, validation_data=(X_test_de, y_test_de), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions\n",
    "y_pred_fr = model_fr.predict(X_test_fr)\n",
    "y_pred_de = model_de.predict(X_test_de)\n",
    "print(y_pred_fr.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatage des r√©sultats pour la soumission\n",
    "y_pred_fr = pd.DataFrame(y_pred_fr, columns=['TARGET'])\n",
    "y_pred_de = pd.DataFrame(y_pred_de, columns=['TARGET'])\n",
    "#y_pred_fr['ID'] = start_df_fr_test['ID'].values\n",
    "#y_pred_de['ID'] = start_df_de_test['ID'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fr['ID'] = X_test_fr['ID'].values\n",
    "y_pred_de['ID'] = X_test_de['ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changer l'ordre des colonnes pour la soumission\n",
    "y_pred_fr = y_pred_fr[['ID', 'TARGET']]\n",
    "y_pred_de = y_pred_de[['ID', 'TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combiner les pr√©dictions des deux pays\n",
    "#y_pred = pd.concat([y_pred_fr, y_pred_de], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#on s'en fou\n",
    "print(f\"Taille de y_pred_fr : {y_pred_fr['TARGET'].shape}\")\n",
    "print(f\"Taille de y_fr : {y_fr.shape}\")\n",
    "print(f\"Taille de y_pred_de : {y_pred_de['TARGET'].shape}\")\n",
    "print(f\"Taille de y_de : {y_de.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Corr√©lation de Spearman\n",
    "# Calculer la corr√©lation de Spearman pour la France\n",
    "correlation_fr, _ = spearmanr(y_pred_fr['TARGET'], y_test_fr)\n",
    "\n",
    "# Calculer la corr√©lation de Spearman pour l'Allemagne\n",
    "correlation_de, _ = spearmanr(y_pred_de['TARGET'], y_test_de)\n",
    "\n",
    "# Affichage des r√©sultats de la corr√©lation de Spearman\n",
    "print(f\"Corr√©lation de Spearman pour la France : {correlation_fr:.4f}\")\n",
    "print(f\"Corr√©lation de Spearman pour l'Allemagne : {correlation_de:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
